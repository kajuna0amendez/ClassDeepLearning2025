{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39ff638-227c-4aea-9407-80c96be905d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe9bbf-1d56-4d12-ab29-a0fab4f4520c",
   "metadata": {},
   "source": [
    "# Adaptive Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa1e1c2-e3b3-49e4-bdfb-5970d561e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100, batch_size=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) for least squares optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (m, n) - input features\n",
    "    - y: numpy array of shape (m,) - target values\n",
    "    - learning_rate: float - step size for parameter updates\n",
    "    - epochs: int - number of full passes over the dataset\n",
    "    - batch_size: int - number of samples per mini-batch\n",
    "    - verbose: bool - whether to print progress\n",
    "\n",
    "    Returns:\n",
    "    - theta: numpy array - optimized parameters\n",
    "    - losses: list - loss values over epochs\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # Initialize model parameters\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # Mini-batch update\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Compute gradient: X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            gradient = X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            theta -= learning_rate * gradient  # Update parameters\n",
    "\n",
    "        # Compute loss on the full dataset\n",
    "        loss = 0.5 * np.mean((y - X @ theta) ** 2)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "    return theta, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537b3b4d-f806-479f-b6f2-4755c39cfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "true_theta = np.array([3.0, 5.0])  # True parameters\n",
    "y = X @ true_theta + np.random.normal(0, 0.1, 100)  # Noisy target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ea28eb-c8b2-48e3-940b-ef717df1c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.921508\n",
      "Epoch 10, Loss: 0.018266\n",
      "Epoch 20, Loss: 0.007115\n",
      "Epoch 30, Loss: 0.005305\n",
      "Epoch 40, Loss: 0.005004\n",
      "Epoch 50, Loss: 0.004956\n",
      "Epoch 60, Loss: 0.004948\n",
      "Epoch 70, Loss: 0.004946\n",
      "Epoch 80, Loss: 0.004946\n",
      "Epoch 90, Loss: 0.004946\n",
      "Epoch 100, Loss: 0.004946\n",
      "Epoch 110, Loss: 0.004946\n",
      "Epoch 120, Loss: 0.004946\n",
      "Epoch 130, Loss: 0.004946\n",
      "Epoch 140, Loss: 0.004948\n",
      "Epoch 150, Loss: 0.004946\n",
      "Epoch 160, Loss: 0.004947\n",
      "Epoch 170, Loss: 0.004947\n",
      "Epoch 180, Loss: 0.004948\n",
      "Epoch 190, Loss: 0.004946\n",
      "Epoch 200, Loss: 0.004947\n",
      "Epoch 210, Loss: 0.004947\n",
      "Epoch 220, Loss: 0.004946\n",
      "Epoch 230, Loss: 0.004946\n",
      "Epoch 240, Loss: 0.004946\n",
      "Epoch 250, Loss: 0.004946\n",
      "Epoch 260, Loss: 0.004946\n",
      "Epoch 270, Loss: 0.004946\n",
      "Epoch 280, Loss: 0.004946\n",
      "Epoch 290, Loss: 0.004946\n",
      "Epoch 300, Loss: 0.004946\n",
      "Epoch 310, Loss: 0.004947\n",
      "Epoch 320, Loss: 0.004947\n",
      "Epoch 330, Loss: 0.004946\n",
      "Epoch 340, Loss: 0.004947\n",
      "Epoch 350, Loss: 0.004946\n",
      "Epoch 360, Loss: 0.004946\n",
      "Epoch 370, Loss: 0.004946\n",
      "Epoch 380, Loss: 0.004946\n",
      "Epoch 390, Loss: 0.004946\n",
      "Epoch 400, Loss: 0.004946\n",
      "Epoch 410, Loss: 0.004946\n",
      "Epoch 420, Loss: 0.004947\n",
      "Epoch 430, Loss: 0.004946\n",
      "Epoch 440, Loss: 0.004946\n",
      "Epoch 450, Loss: 0.004946\n",
      "Epoch 460, Loss: 0.004946\n",
      "Epoch 470, Loss: 0.004948\n",
      "Epoch 480, Loss: 0.004947\n",
      "Epoch 490, Loss: 0.004946\n",
      "Epoch 500, Loss: 0.004947\n",
      "Epoch 510, Loss: 0.004947\n",
      "Epoch 520, Loss: 0.004947\n",
      "Epoch 530, Loss: 0.004946\n",
      "Epoch 540, Loss: 0.004946\n",
      "Epoch 550, Loss: 0.004947\n",
      "Epoch 560, Loss: 0.004946\n",
      "Epoch 570, Loss: 0.004946\n",
      "Epoch 580, Loss: 0.004946\n",
      "Epoch 590, Loss: 0.004947\n",
      "Epoch 600, Loss: 0.004946\n",
      "Epoch 610, Loss: 0.004946\n",
      "Epoch 620, Loss: 0.004948\n",
      "Epoch 630, Loss: 0.004946\n",
      "Epoch 640, Loss: 0.004946\n",
      "Epoch 650, Loss: 0.004947\n",
      "Epoch 660, Loss: 0.004946\n",
      "Epoch 670, Loss: 0.004946\n",
      "Epoch 680, Loss: 0.004946\n",
      "Epoch 690, Loss: 0.004946\n",
      "Epoch 700, Loss: 0.004946\n",
      "Epoch 710, Loss: 0.004947\n",
      "Epoch 720, Loss: 0.004947\n",
      "Epoch 730, Loss: 0.004947\n",
      "Epoch 740, Loss: 0.004946\n",
      "Epoch 750, Loss: 0.004946\n",
      "Epoch 760, Loss: 0.004946\n",
      "Epoch 770, Loss: 0.004946\n",
      "Epoch 780, Loss: 0.004946\n",
      "Epoch 790, Loss: 0.004946\n",
      "Epoch 800, Loss: 0.004947\n",
      "Epoch 810, Loss: 0.004946\n",
      "Epoch 820, Loss: 0.004946\n",
      "Epoch 830, Loss: 0.004946\n",
      "Epoch 840, Loss: 0.004946\n",
      "Epoch 850, Loss: 0.004946\n",
      "Epoch 860, Loss: 0.004946\n",
      "Epoch 870, Loss: 0.004947\n",
      "Epoch 880, Loss: 0.004946\n",
      "Epoch 890, Loss: 0.004946\n",
      "Epoch 900, Loss: 0.004946\n",
      "Epoch 910, Loss: 0.004946\n",
      "Epoch 920, Loss: 0.004946\n",
      "Epoch 930, Loss: 0.004947\n",
      "Epoch 940, Loss: 0.004946\n",
      "Epoch 950, Loss: 0.004946\n",
      "Epoch 960, Loss: 0.004946\n",
      "Epoch 970, Loss: 0.004946\n",
      "Epoch 980, Loss: 0.004946\n",
      "Epoch 990, Loss: 0.004947\n",
      "Optimized parameters: [3.0135622  5.01259119]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD\n",
    "theta, losses = stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=1000, batch_size=10)\n",
    "print(\"Optimized parameters:\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff6ffb4-1a16-4221-9c7f-14a0aa40f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_adaptive(X, y, epochs=100, batch_size=10, alpha=0.00000001, verbose=True):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) for least squares optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (m, n) - input features\n",
    "    - y: numpy array of shape (m,) - target values\n",
    "    - learning_rate: float - step size for parameter updates\n",
    "    - epochs: int - number of full passes over the dataset\n",
    "    - batch_size: int - number of samples per mini-batch\n",
    "    - verbose: bool - whether to print progress\n",
    "\n",
    "    Returns:\n",
    "    - theta: numpy array - optimized parameters\n",
    "    - losses: list - loss values over epochs\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # Initialize model parameters\n",
    "    losses = []\n",
    "    learning_rate = 0.01\n",
    "    past_theta = np.array([0.1, 0.1])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        \n",
    "        # Mini-batch update\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Compute gradient: X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            gradient = X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            theta = theta - learning_rate * gradient  # Update parameters\n",
    "            new_gradient =  X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            learning_rate = learning_rate - alpha * gradient @ new_gradient.T\n",
    "             \n",
    "\n",
    "        # Compute loss on the full dataset\n",
    "        loss = 0.5 * np.mean((y - X @ theta) ** 2)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            print(f\"Epoch {epoch}, Learning_Rate: {learning_rate:.6f}\")\n",
    "\n",
    "    return theta, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9897a2ff-70e8-49d4-bcc9-9481e7a48bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.946068\n",
      "Epoch 0, Learning_Rate: 0.009937\n",
      "Epoch 10, Loss: 0.017582\n",
      "Epoch 10, Learning_Rate: 0.009906\n",
      "Epoch 20, Loss: 0.007047\n",
      "Epoch 20, Learning_Rate: 0.009906\n",
      "Epoch 30, Loss: 0.005296\n",
      "Epoch 30, Learning_Rate: 0.009906\n",
      "Epoch 40, Loss: 0.005005\n",
      "Epoch 40, Learning_Rate: 0.009906\n",
      "Epoch 50, Loss: 0.004957\n",
      "Epoch 50, Learning_Rate: 0.009906\n",
      "Epoch 60, Loss: 0.004947\n",
      "Epoch 60, Learning_Rate: 0.009906\n",
      "Epoch 70, Loss: 0.004946\n",
      "Epoch 70, Learning_Rate: 0.009906\n",
      "Epoch 80, Loss: 0.004947\n",
      "Epoch 80, Learning_Rate: 0.009905\n",
      "Epoch 90, Loss: 0.004947\n",
      "Epoch 90, Learning_Rate: 0.009905\n",
      "Epoch 100, Loss: 0.004946\n",
      "Epoch 100, Learning_Rate: 0.009905\n",
      "Epoch 110, Loss: 0.004946\n",
      "Epoch 110, Learning_Rate: 0.009905\n",
      "Epoch 120, Loss: 0.004946\n",
      "Epoch 120, Learning_Rate: 0.009905\n",
      "Epoch 130, Loss: 0.004946\n",
      "Epoch 130, Learning_Rate: 0.009905\n",
      "Epoch 140, Loss: 0.004946\n",
      "Epoch 140, Learning_Rate: 0.009905\n",
      "Epoch 150, Loss: 0.004946\n",
      "Epoch 150, Learning_Rate: 0.009905\n",
      "Epoch 160, Loss: 0.004947\n",
      "Epoch 160, Learning_Rate: 0.009905\n",
      "Epoch 170, Loss: 0.004946\n",
      "Epoch 170, Learning_Rate: 0.009905\n",
      "Epoch 180, Loss: 0.004947\n",
      "Epoch 180, Learning_Rate: 0.009905\n",
      "Epoch 190, Loss: 0.004947\n",
      "Epoch 190, Learning_Rate: 0.009905\n",
      "Epoch 200, Loss: 0.004949\n",
      "Epoch 200, Learning_Rate: 0.009905\n",
      "Epoch 210, Loss: 0.004947\n",
      "Epoch 210, Learning_Rate: 0.009905\n",
      "Epoch 220, Loss: 0.004947\n",
      "Epoch 220, Learning_Rate: 0.009904\n",
      "Epoch 230, Loss: 0.004946\n",
      "Epoch 230, Learning_Rate: 0.009904\n",
      "Epoch 240, Loss: 0.004947\n",
      "Epoch 240, Learning_Rate: 0.009904\n",
      "Epoch 250, Loss: 0.004946\n",
      "Epoch 250, Learning_Rate: 0.009904\n",
      "Epoch 260, Loss: 0.004948\n",
      "Epoch 260, Learning_Rate: 0.009904\n",
      "Epoch 270, Loss: 0.004946\n",
      "Epoch 270, Learning_Rate: 0.009904\n",
      "Epoch 280, Loss: 0.004946\n",
      "Epoch 280, Learning_Rate: 0.009904\n",
      "Epoch 290, Loss: 0.004946\n",
      "Epoch 290, Learning_Rate: 0.009904\n",
      "Epoch 300, Loss: 0.004946\n",
      "Epoch 300, Learning_Rate: 0.009904\n",
      "Epoch 310, Loss: 0.004946\n",
      "Epoch 310, Learning_Rate: 0.009904\n",
      "Epoch 320, Loss: 0.004947\n",
      "Epoch 320, Learning_Rate: 0.009904\n",
      "Epoch 330, Loss: 0.004946\n",
      "Epoch 330, Learning_Rate: 0.009904\n",
      "Epoch 340, Loss: 0.004946\n",
      "Epoch 340, Learning_Rate: 0.009904\n",
      "Epoch 350, Loss: 0.004946\n",
      "Epoch 350, Learning_Rate: 0.009904\n",
      "Epoch 360, Loss: 0.004946\n",
      "Epoch 360, Learning_Rate: 0.009903\n",
      "Epoch 370, Loss: 0.004946\n",
      "Epoch 370, Learning_Rate: 0.009903\n",
      "Epoch 380, Loss: 0.004946\n",
      "Epoch 380, Learning_Rate: 0.009903\n",
      "Epoch 390, Loss: 0.004946\n",
      "Epoch 390, Learning_Rate: 0.009903\n",
      "Epoch 400, Loss: 0.004946\n",
      "Epoch 400, Learning_Rate: 0.009903\n",
      "Epoch 410, Loss: 0.004947\n",
      "Epoch 410, Learning_Rate: 0.009903\n",
      "Epoch 420, Loss: 0.004947\n",
      "Epoch 420, Learning_Rate: 0.009903\n",
      "Epoch 430, Loss: 0.004947\n",
      "Epoch 430, Learning_Rate: 0.009903\n",
      "Epoch 440, Loss: 0.004946\n",
      "Epoch 440, Learning_Rate: 0.009903\n",
      "Epoch 450, Loss: 0.004947\n",
      "Epoch 450, Learning_Rate: 0.009903\n",
      "Epoch 460, Loss: 0.004946\n",
      "Epoch 460, Learning_Rate: 0.009903\n",
      "Epoch 470, Loss: 0.004946\n",
      "Epoch 470, Learning_Rate: 0.009903\n",
      "Epoch 480, Loss: 0.004946\n",
      "Epoch 480, Learning_Rate: 0.009903\n",
      "Epoch 490, Loss: 0.004947\n",
      "Epoch 490, Learning_Rate: 0.009903\n",
      "Epoch 500, Loss: 0.004946\n",
      "Epoch 500, Learning_Rate: 0.009903\n",
      "Epoch 510, Loss: 0.004946\n",
      "Epoch 510, Learning_Rate: 0.009902\n",
      "Epoch 520, Loss: 0.004946\n",
      "Epoch 520, Learning_Rate: 0.009902\n",
      "Epoch 530, Loss: 0.004946\n",
      "Epoch 530, Learning_Rate: 0.009902\n",
      "Epoch 540, Loss: 0.004946\n",
      "Epoch 540, Learning_Rate: 0.009902\n",
      "Epoch 550, Loss: 0.004946\n",
      "Epoch 550, Learning_Rate: 0.009902\n",
      "Epoch 560, Loss: 0.004946\n",
      "Epoch 560, Learning_Rate: 0.009902\n",
      "Epoch 570, Loss: 0.004946\n",
      "Epoch 570, Learning_Rate: 0.009902\n",
      "Epoch 580, Loss: 0.004946\n",
      "Epoch 580, Learning_Rate: 0.009902\n",
      "Epoch 590, Loss: 0.004946\n",
      "Epoch 590, Learning_Rate: 0.009902\n",
      "Epoch 600, Loss: 0.004946\n",
      "Epoch 600, Learning_Rate: 0.009902\n",
      "Epoch 610, Loss: 0.004946\n",
      "Epoch 610, Learning_Rate: 0.009902\n",
      "Epoch 620, Loss: 0.004946\n",
      "Epoch 620, Learning_Rate: 0.009902\n",
      "Epoch 630, Loss: 0.004946\n",
      "Epoch 630, Learning_Rate: 0.009902\n",
      "Epoch 640, Loss: 0.004948\n",
      "Epoch 640, Learning_Rate: 0.009902\n",
      "Epoch 650, Loss: 0.004948\n",
      "Epoch 650, Learning_Rate: 0.009901\n",
      "Epoch 660, Loss: 0.004946\n",
      "Epoch 660, Learning_Rate: 0.009901\n",
      "Epoch 670, Loss: 0.004948\n",
      "Epoch 670, Learning_Rate: 0.009901\n",
      "Epoch 680, Loss: 0.004946\n",
      "Epoch 680, Learning_Rate: 0.009901\n",
      "Epoch 690, Loss: 0.004946\n",
      "Epoch 690, Learning_Rate: 0.009901\n",
      "Epoch 700, Loss: 0.004946\n",
      "Epoch 700, Learning_Rate: 0.009901\n",
      "Epoch 710, Loss: 0.004946\n",
      "Epoch 710, Learning_Rate: 0.009901\n",
      "Epoch 720, Loss: 0.004946\n",
      "Epoch 720, Learning_Rate: 0.009901\n",
      "Epoch 730, Loss: 0.004946\n",
      "Epoch 730, Learning_Rate: 0.009901\n",
      "Epoch 740, Loss: 0.004946\n",
      "Epoch 740, Learning_Rate: 0.009901\n",
      "Epoch 750, Loss: 0.004948\n",
      "Epoch 750, Learning_Rate: 0.009901\n",
      "Epoch 760, Loss: 0.004947\n",
      "Epoch 760, Learning_Rate: 0.009901\n",
      "Epoch 770, Loss: 0.004946\n",
      "Epoch 770, Learning_Rate: 0.009901\n",
      "Epoch 780, Loss: 0.004946\n",
      "Epoch 780, Learning_Rate: 0.009901\n",
      "Epoch 790, Loss: 0.004946\n",
      "Epoch 790, Learning_Rate: 0.009900\n",
      "Epoch 800, Loss: 0.004946\n",
      "Epoch 800, Learning_Rate: 0.009900\n",
      "Epoch 810, Loss: 0.004946\n",
      "Epoch 810, Learning_Rate: 0.009900\n",
      "Epoch 820, Loss: 0.004946\n",
      "Epoch 820, Learning_Rate: 0.009900\n",
      "Epoch 830, Loss: 0.004946\n",
      "Epoch 830, Learning_Rate: 0.009900\n",
      "Epoch 840, Loss: 0.004946\n",
      "Epoch 840, Learning_Rate: 0.009900\n",
      "Epoch 850, Loss: 0.004946\n",
      "Epoch 850, Learning_Rate: 0.009900\n",
      "Epoch 860, Loss: 0.004946\n",
      "Epoch 860, Learning_Rate: 0.009900\n",
      "Epoch 870, Loss: 0.004947\n",
      "Epoch 870, Learning_Rate: 0.009900\n",
      "Epoch 880, Loss: 0.004946\n",
      "Epoch 880, Learning_Rate: 0.009900\n",
      "Epoch 890, Loss: 0.004946\n",
      "Epoch 890, Learning_Rate: 0.009900\n",
      "Epoch 900, Loss: 0.004946\n",
      "Epoch 900, Learning_Rate: 0.009900\n",
      "Epoch 910, Loss: 0.004947\n",
      "Epoch 910, Learning_Rate: 0.009900\n",
      "Epoch 920, Loss: 0.004946\n",
      "Epoch 920, Learning_Rate: 0.009899\n",
      "Epoch 930, Loss: 0.004947\n",
      "Epoch 930, Learning_Rate: 0.009899\n",
      "Epoch 940, Loss: 0.004946\n",
      "Epoch 940, Learning_Rate: 0.009899\n",
      "Epoch 950, Loss: 0.004946\n",
      "Epoch 950, Learning_Rate: 0.009899\n",
      "Epoch 960, Loss: 0.004946\n",
      "Epoch 960, Learning_Rate: 0.009899\n",
      "Epoch 970, Loss: 0.004948\n",
      "Epoch 970, Learning_Rate: 0.009899\n",
      "Epoch 980, Loss: 0.004946\n",
      "Epoch 980, Learning_Rate: 0.009899\n",
      "Epoch 990, Loss: 0.004946\n",
      "Epoch 990, Learning_Rate: 0.009899\n",
      "Optimized parameters: [3.01536898 5.01373417]\n"
     ]
    }
   ],
   "source": [
    "# Run SGD Adaptive\n",
    "theta, losses = stochastic_gradient_descent_adaptive(X, y, epochs=1000, batch_size=10)\n",
    "print(\"Optimized parameters:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9492b-48df-4655-96aa-c1b483ef6be5",
   "metadata": {},
   "source": [
    "# AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9beb4ea-46d0-4385-b557-1ea0b8e4aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.812798\n",
      "Epoch 10, Loss: 2.886682\n",
      "Epoch 20, Loss: 1.647228\n",
      "Epoch 30, Loss: 1.014884\n",
      "Epoch 40, Loss: 0.653440\n",
      "Epoch 50, Loss: 0.435936\n",
      "Epoch 60, Loss: 0.300977\n",
      "Epoch 70, Loss: 0.215429\n",
      "Epoch 80, Loss: 0.160195\n",
      "Epoch 90, Loss: 0.123868\n",
      "Epoch 100, Loss: 0.099478\n",
      "Epoch 110, Loss: 0.082711\n",
      "Epoch 120, Loss: 0.070841\n",
      "Epoch 130, Loss: 0.062152\n",
      "Epoch 140, Loss: 0.055567\n",
      "Epoch 150, Loss: 0.050388\n",
      "Epoch 160, Loss: 0.046172\n",
      "Epoch 170, Loss: 0.042633\n",
      "Epoch 180, Loss: 0.039584\n",
      "Epoch 190, Loss: 0.036903\n",
      "Epoch 200, Loss: 0.034507\n",
      "Epoch 210, Loss: 0.032340\n",
      "Epoch 220, Loss: 0.030365\n",
      "Epoch 230, Loss: 0.028553\n",
      "Epoch 240, Loss: 0.026884\n",
      "Epoch 250, Loss: 0.025342\n",
      "Epoch 260, Loss: 0.023913\n",
      "Epoch 270, Loss: 0.022589\n",
      "Epoch 280, Loss: 0.021359\n",
      "Epoch 290, Loss: 0.020217\n",
      "Epoch 300, Loss: 0.019155\n",
      "Epoch 310, Loss: 0.018168\n",
      "Epoch 320, Loss: 0.017250\n",
      "Epoch 330, Loss: 0.016396\n",
      "Epoch 340, Loss: 0.015601\n",
      "Epoch 350, Loss: 0.014862\n",
      "Epoch 360, Loss: 0.014174\n",
      "Epoch 370, Loss: 0.013535\n",
      "Epoch 380, Loss: 0.012939\n",
      "Epoch 390, Loss: 0.012385\n",
      "Epoch 400, Loss: 0.011870\n",
      "Epoch 410, Loss: 0.011390\n",
      "Epoch 420, Loss: 0.010943\n",
      "Epoch 430, Loss: 0.010528\n",
      "Epoch 440, Loss: 0.010141\n",
      "Epoch 450, Loss: 0.009781\n",
      "Epoch 460, Loss: 0.009446\n",
      "Epoch 470, Loss: 0.009135\n",
      "Epoch 480, Loss: 0.008845\n",
      "Epoch 490, Loss: 0.008575\n",
      "Epoch 500, Loss: 0.008324\n",
      "Epoch 510, Loss: 0.008090\n",
      "Epoch 520, Loss: 0.007872\n",
      "Epoch 530, Loss: 0.007670\n",
      "Epoch 540, Loss: 0.007481\n",
      "Epoch 550, Loss: 0.007306\n",
      "Epoch 560, Loss: 0.007142\n",
      "Epoch 570, Loss: 0.006990\n",
      "Epoch 580, Loss: 0.006849\n",
      "Epoch 590, Loss: 0.006717\n",
      "Epoch 600, Loss: 0.006595\n",
      "Epoch 610, Loss: 0.006480\n",
      "Epoch 620, Loss: 0.006374\n",
      "Epoch 630, Loss: 0.006275\n",
      "Epoch 640, Loss: 0.006183\n",
      "Epoch 650, Loss: 0.006098\n",
      "Epoch 660, Loss: 0.006018\n",
      "Epoch 670, Loss: 0.005944\n",
      "Epoch 680, Loss: 0.005875\n",
      "Epoch 690, Loss: 0.005811\n",
      "Epoch 700, Loss: 0.005751\n",
      "Epoch 710, Loss: 0.005695\n",
      "Epoch 720, Loss: 0.005643\n",
      "Epoch 730, Loss: 0.005595\n",
      "Epoch 740, Loss: 0.005550\n",
      "Epoch 750, Loss: 0.005508\n",
      "Epoch 760, Loss: 0.005470\n",
      "Epoch 770, Loss: 0.005433\n",
      "Epoch 780, Loss: 0.005400\n",
      "Epoch 790, Loss: 0.005368\n",
      "Epoch 800, Loss: 0.005339\n",
      "Epoch 810, Loss: 0.005312\n",
      "Epoch 820, Loss: 0.005287\n",
      "Epoch 830, Loss: 0.005263\n",
      "Epoch 840, Loss: 0.005241\n",
      "Epoch 850, Loss: 0.005221\n",
      "Epoch 860, Loss: 0.005202\n",
      "Epoch 870, Loss: 0.005184\n",
      "Epoch 880, Loss: 0.005168\n",
      "Epoch 890, Loss: 0.005152\n",
      "Epoch 900, Loss: 0.005138\n",
      "Epoch 910, Loss: 0.005125\n",
      "Epoch 920, Loss: 0.005112\n",
      "Epoch 930, Loss: 0.005101\n",
      "Epoch 940, Loss: 0.005090\n",
      "Epoch 950, Loss: 0.005080\n",
      "Epoch 960, Loss: 0.005071\n",
      "Epoch 970, Loss: 0.005062\n",
      "Epoch 980, Loss: 0.005054\n",
      "Epoch 990, Loss: 0.005047\n",
      "Optimized parameters: [3.04694699 4.98090313]\n"
     ]
    }
   ],
   "source": [
    "def adagrad(X, y, learning_rate=0.01, epochs=100, batch_size=10, verbose=True, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Adagrad optimization for least squares regression.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (m, n) - input features\n",
    "    - y: numpy array of shape (m,) - target values\n",
    "    - learning_rate: float - initial learning rate\n",
    "    - epochs: int - number of full passes over the dataset\n",
    "    - batch_size: int - number of samples per mini-batch\n",
    "    - verbose: bool - whether to print progress\n",
    "    - epsilon: float - small constant to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "    - theta: numpy array - optimized parameters\n",
    "    - losses: list - loss values over epochs\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # Initialize model parameters\n",
    "    sum_grads = np.zeros(n)  # Sum of squared gradients\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # Mini-batch update\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Compute gradient: X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            gradient = X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            sum_grads += gradient ** 2  # Accumulate squared gradients\n",
    "\n",
    "            # Adaptive learning rate per parameter\n",
    "            adaptive_lr = learning_rate / (np.sqrt(sum_grads) + epsilon)\n",
    "\n",
    "            # Update parameters\n",
    "            theta -= adaptive_lr * gradient\n",
    "\n",
    "        # Compute loss on the full dataset\n",
    "        loss = 0.5 * np.mean((y - X @ theta) ** 2)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "    return theta, losses\n",
    "\n",
    "\n",
    "# Example usage: Synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "true_theta = np.array([3.0, 5.0])  # True parameters\n",
    "y = X @ true_theta + np.random.normal(0, 0.1, 100)  # Noisy target\n",
    "\n",
    "# Run Adagrad\n",
    "theta, losses = adagrad(X, y, learning_rate=0.1, epochs=1000, batch_size=10)\n",
    "print(\"Optimized parameters:\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bd6a8-081d-48d7-b6b0-a2b702432553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
